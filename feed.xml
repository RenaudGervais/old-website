<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Renaud Gervais</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://renaudgervais.github.io/feed.xml" />
<link rel="alternate" type="text/html" href="http://renaudgervais.github.io" />
<updated>2015-10-22T19:34:47+02:00</updated>
<id>http://renaudgervais.github.io/</id>
<author>
  <name>Renaud Gervais</name>
  <uri>http://renaudgervais.github.io/</uri>
  <email>renaud.gervais@gmail.com</email>
</author>


  

<entry>
  <title type="html"><![CDATA[Tobe: Tangible Out-of-Body Experience]]></title>
  <link rel="alternate" type="text/html" href="http://renaudgervais.github.io/tobe/" />
  <id>http://renaudgervais.github.io/tobe</id>
  <published>2015-10-22T16:08:46+02:00</published>
  <updated>2015-10-22T16:08:46+02:00</updated>
  <author>
    <name>Renaud Gervais</name>
    <uri>http://renaudgervais.github.io</uri>
    <email>renaud.gervais@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;/teegi-tangible-eeg-interface/&quot;&gt;Teegi&lt;/a&gt; was our first attempt at merging tangible avatars and physiological computing. However, it was limited to EEG signals. We wanted to go further than raw signals and give &lt;em&gt;freedom&lt;/em&gt; to the users, inviting them to create and define their own tangible self-representations, to gain insights on their inner selves.&lt;/p&gt;

&lt;p&gt;With Tobe, a toolkit to create Tangible Out-of-Body Experiences, we propose a toolkit that lets users explore their physiological states on their own. We cover the tangible avatar creation (through 3D printing), the physiological sensors creation (such as EDA, ECG and EEG leveraging open hardware solutions – &lt;a href=&quot;http://www.bitalino.com&quot;&gt;Bitalino&lt;/a&gt; and &lt;a href=&quot;http://www.openbci.com/&quot;&gt;OpenBCI&lt;/a&gt; boards), the signal processing pipeline (using &lt;a href=&quot;http://openvibe.inria.fr/&quot;&gt;OpenViBE&lt;/a&gt;) and the visual representations of the signals (using &lt;a href=&quot;http://vvvv.org&quot;&gt;vvvv&lt;/a&gt;). We hope that this toolkit could eventually be used to foster a better understanding of our own selves, fostering introspection, and create empathy for others.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/142287968&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;related-publications&quot;&gt;Related publications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span id=&quot;Gervais2016a&quot;&gt;Gervais, Renaud, Jérémy Frey, Alexis Gay, Fabien Lotte, and Martin Hachet. “TOBE: Tangible Out-Of-Body Experience.” In &lt;i&gt;Proceedings Of the 10th International Conference on Tangible, Embedded and Embodied Interaction&lt;/i&gt;. TEI ’16. Eindhoven, Netherlands: ACM, 2016. https://hal.archives-ouvertes.fr/hal-01215499.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://renaudgervais.github.io/tobe/&quot;&gt;Tobe: Tangible Out-of-Body Experience&lt;/a&gt; was originally published by Renaud Gervais at &lt;a href=&quot;http://renaudgervais.github.io&quot;&gt;Renaud Gervais&lt;/a&gt; on October 22, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Tangible Viewports]]></title>
  <link rel="alternate" type="text/html" href="http://renaudgervais.github.io/tangible-viewports/" />
  <id>http://renaudgervais.github.io/tangible-viewports</id>
  <published>2015-10-21T18:50:28+02:00</published>
  <updated>2015-10-21T18:50:28+02:00</updated>
  <author>
    <name>Renaud Gervais</name>
    <uri>http://renaudgervais.github.io</uri>
    <email>renaud.gervais@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;Don’t you feel trapped when working on a screen? On our desks, we often have two different workspaces: one for working digitally (screens) and one for working physically (sketches, post-its, prototypes). We think that this dichotomy is &lt;em&gt;limiting&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We proposed to link the two working spaces using a metaphor we called &lt;em&gt;Tangible Viewports&lt;/em&gt; and is a direct follow-up to the &lt;a href=&quot;/cursar-pointing-in-spatial-augmented-reality-from-2d-pointing-devices/&quot;&gt;CurSAR&lt;/a&gt; project.. It consists in an on-screen window where, when a physical object is held in front of it (from the point of view of the user), the cursor on screen “jumps” onto the physical object. Combined with Spatial Augmented Reality, it allowed us to connect to existing desktop applications, such as &lt;a href=&quot;http://www.adobe.com/fr/products/photoshop.html&quot;&gt;Photoshop&lt;/a&gt;, to real objects.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/142358002&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;related-publications&quot;&gt;Related publications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span id=&quot;Gervais2016&quot;&gt;Gervais, Renaud, Joan Sol Roo, and Martin Hachet. “Tangible Viewports: Getting Out Of Flatland in Desktop Environments.” In &lt;i&gt;Proceedings Of the 10th International Conference on Tangible, Embedded and Embodied Interaction&lt;/i&gt;. TEI ’16. Eindhoven, Netherlands: ACM, 2016. https://hal.archives-ouvertes.fr/hal-01215502.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://renaudgervais.github.io/tangible-viewports/&quot;&gt;Tangible Viewports&lt;/a&gt; was originally published by Renaud Gervais at &lt;a href=&quot;http://renaudgervais.github.io&quot;&gt;Renaud Gervais&lt;/a&gt; on October 21, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[CurSAR: Pointing in Spatial Augmented Reality from 2D Pointing Devices]]></title>
  <link rel="alternate" type="text/html" href="http://renaudgervais.github.io/cursar-pointing-in-spatial-augmented-reality-from-2d-pointing-devices/" />
  <id>http://renaudgervais.github.io/cursar-pointing-in-spatial-augmented-reality-from-2d-pointing-devices</id>
  <updated>2015-06-08 13:03:11 +0200T00:00:00-00:00</updated>
  <published>2015-06-08T00:00:00+02:00</published>
  
  <author>
    <name>Renaud Gervais</name>
    <uri>http://renaudgervais.github.io</uri>
    <email>renaud.gervais@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;CurSAR is a project investigating the use of 2D input devices to point at augmented physical objects. The main goal of the study was to compare the performance of a pointing task in a SAR and SCREEN condition. We created an experimental setup that allowed to have the same view of augmented objects either physical (SAR) or virtual (SCREEN).  Participants were 11% slower in the SAR condition. However, the transfer function of the mouse to the cursor, even without the physical presence of a screen as a reference system for the cursor, continued to the be effective.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/cursar-setup.png&quot;&gt;&lt;img src=&quot;/images/cursar-setup.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-publications&quot;&gt;Related publications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span id=&quot;Gervais2015&quot;&gt;Gervais, Renaud, Jérémy Frey, and Martin Hachet. “Pointing In Spatial Augmented Reality from 2D Pointing Devices.” In &lt;i&gt;INTERACT&lt;/i&gt;, 8. Bamberg, Germany, 2015. https://hal.archives-ouvertes.fr/hal-01153647.&lt;/span&gt; &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01153647/document&quot;&gt;&lt;img src=&quot;/images/pdf.png&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://renaudgervais.github.io/cursar-pointing-in-spatial-augmented-reality-from-2d-pointing-devices/&quot;&gt;CurSAR: Pointing in Spatial Augmented Reality from 2D Pointing Devices&lt;/a&gt; was originally published by Renaud Gervais at &lt;a href=&quot;http://renaudgervais.github.io&quot;&gt;Renaud Gervais&lt;/a&gt; on June 08, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Teegi: Tangible EEG Interface]]></title>
  <link rel="alternate" type="text/html" href="http://renaudgervais.github.io/teegi-tangible-eeg-interface/" />
  <id>http://renaudgervais.github.io/teegi-tangible-eeg-interface</id>
  <updated>2014-07-17 16:13:52 +0200T00:00:00-00:00</updated>
  <published>2014-07-17T00:00:00+02:00</published>
  
  <author>
    <name>Renaud Gervais</name>
    <uri>http://renaudgervais.github.io</uri>
    <email>renaud.gervais@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;Teegi is a Tangible EEG (ElectroEncephaloGraphy) Interface. It uses a physical puppet on which your brain activity is displayed in real-time.&lt;/p&gt;

&lt;script language=&quot;VVVV&quot; src=&quot;../assets/patches/teegi.v4p&quot;&gt;&lt;/script&gt;

&lt;div id=&quot;renderer&quot; class=&quot;image-pull-right&quot;&gt;&lt;/div&gt;

&lt;p&gt;The main objective with Teegi was to enable novice users to get access to tools and visualizations that usually only experts have access to. By making the installation tangible, users can focus on the effect of certain activities (&lt;em&gt;e.g.&lt;/em&gt; moving your hands or feet, closing your eyes or trying to relax/meditate) on the readings of their brain activity and therefore increasing their understanding of cerebral processes.&lt;/p&gt;

&lt;p&gt;We think Teegi could be a tool very well suited to education purposes (&lt;em&gt;e.g.&lt;/em&gt; scientific museums) or for use in BCI (Brain Computer Interfaces) training.&lt;/p&gt;

&lt;iframe src=&quot;//player.vimeo.com/video/104486980&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;related-publications&quot;&gt;Related publications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span id=&quot;Frey2014&quot;&gt;Frey, Jérémy, Renaud Gervais, Stéphanie Fleck, Fabien Lotte, and Martin Hachet. “Teegi: Tangible EEG Interface.” In &lt;i&gt;Proceedings Of the 27th Annual ACM Symposium on User Interface Software and Technology&lt;/i&gt;. UIST ’14. ACM, 2014.&lt;/span&gt; &lt;a href=&quot;/papers/Frey2014.pdf&quot;&gt;&lt;img src=&quot;/images/pdf.png&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://renaudgervais.github.io/UIST2014-teegi-presentation/&quot;&gt;Slides&lt;/a&gt; of the talk that &lt;a href=&quot;http://phd.jfrey.info/&quot;&gt;Jeremy&lt;/a&gt; and I gave at UIST 2014&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gizmodo.com/7-experimental-interfaces-that-show-the-future-of-ui-de-1642890943&quot;&gt;Gizmodo&lt;/a&gt;: Teegi has been highlighted as an interface that shows the future of UI design&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://motherboard.vice.com/read/brain-waves-doll&quot;&gt;Motherboard&lt;/a&gt;: A great summary of the Teegi project&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://renaudgervais.github.io/teegi-tangible-eeg-interface/&quot;&gt;Teegi: Tangible EEG Interface&lt;/a&gt; was originally published by Renaud Gervais at &lt;a href=&quot;http://renaudgervais.github.io&quot;&gt;Renaud Gervais&lt;/a&gt; on July 17, 2014.&lt;/p&gt;
  </content>
</entry>

</feed>
